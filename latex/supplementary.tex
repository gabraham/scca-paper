\documentclass[a4paper,10pt]{article}


%\usepackage{a4wide}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[numbers]{natbib}

\geometry{
   includeheadfoot,
   margin=2.54cm
}


\hypersetup{
   pdftitle={},
   pdfauthor={Gad Abraham},
   colorlinks=true,
   citecolor=black,
   filecolor=black,
   linkcolor=black,
   urlcolor=black
}

\input{numbers.tex}

\author{Gad Abraham and Michael Inouye}

\title{FlashPCA: fast sparse canonical correlation analysis of genomic data
--- supplementary material}

\begin{document}

\maketitle

\section{Reproducibility}

Code to reproduce these experiments is at
\url{https://github.com/gabraham/scca-paper}.

\section{HapMap data preprocessing and quality control}

The HapMap3 phase III~\citep{hapmap2010} genotypes were obtained from
\url{ftp://ftp.ncbi.nlm.nih.gov/hapmap/genotypes/2010-05_phaseIII/plink_format/}.
Gene expression levels were obtained from
\url{http://www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-264} and
\url{http//www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-198} (the CEU subset).

Within each of the eight populations (CEU, CHB, GIH, JPT, LWK, MEX,
MKK, YRI), we excluded individuals who were non-founders, had genotyping
missingness~${>}10\%$, or did not have matching gene expression data, resulting
in~\nindiv individuals in total. In addition, within each population, we
excluded non-autosomal SNPs, SNPs with MAF~${<}1\%$, missingness~${>}10\%$,
and deviation from Hardy-Weinberg equilibrium~$P{<}10^{-6}$ using
PLINK~1.9~\citep{purcell2007,Chang2015}.  Finally, we combined all eight
population into one dataset, consisting of the post-QC autosomal SNPs within
each dataset, a total of~\nsnps SNPs.

In the analysis, any remaining missing genotypes were randomly imputed
according to the frequencies of the non-missing observations (equivalent on
average to mean imputation).

For the gene expression data, we used a subset consisting of the~21,800 probes
that were analysed by~\citep{Stranger2012}, utilising the original authors'
normalised data. Following~\citep{Stranger2012}, we performed PCA on the
genotypes within each population, and for the GIH, MEX, MKK, and LWK regressed
out~10 PCs of the genotypes (as well as intercept) from the corresponding gene
expression levels, in order to adjust for the higher levels of admixture within
these populations.  We further filtered probes with low variance
(std.~dev.~${<}0.1$), leaving~\ngenes probes. Both the gene expression levels and
the genotypes were standardised to zero-mean and unit-variance.

\section{Comparison of predictive power with simulated gene expression data}
\label{section:sim}

The number of samples in the HapMap3 data ($n=\nindiv$) does not provide
adequate statistical power to detect weak correlations or difference in
correlations between two competing methods, particularly when cross-validation
further reduces the sample size in the test data (3-fold cross-validation
leads to a test set of~${\sim}236$ individuals). For example, there is power
of only~$34\%$ to detect a correlation~$\rho=0.1$ at an~$\alpha=0.05$
with~236 samples, and only $8\%$ power to detect a difference in
correlations~$\Delta\rho=0.05$ at~$\alpha=0.05$ (i.e., comparing whether
two test-set correlations achieved by competing methods are significantly
different from one another).

% pwr::pwr.r.test for detecting a correlation; stats::power.t.test in R
% for comparing methods

Hence, we chose to simulate gene expression data with strong associations
with the genotypes, allowing for higher correlations to be observed and
meaningfully compared.  Utilising 10,000 SNPs from HapMap3 chromosome~1,
we simulated 1,000 gene expression levels as
$$
\mathbf{Y} = \mathbf{X} \mathbf{B} + \mathbf{E},
$$
where $\mathbf{X}$ are the genotypes ($n \times p$ matrix, $n$ individuals and
$p$ SNPs), $\mathbf{B}$ is a $p \times m$ matrix of weights (effect sizes for
each pair of SNP and phenotype), and is an $n \times m$ matrix representing
the error (noise). To match the sparsity assumptions of SCCA, $\mathbf{B}$
was chosen to be a mixture of weights $\{0.001, 1\}$ with proportions 0.9999
and 0.0001 (across all $n \times m$ entries), respectively.  Note that a value
of 0.001 was used rather than zero, in order prevent some probes from having
zero genetic variance. Each column $k=1,\hdots,m$ of $\mathbf{E}$ was $E_k
\sim \mathcal{N}(0, \frac{1-h^2}{h^2} \mbox{var}((\mathbf{X}\mathbf{B})_k))$,
and $h^2=0.1$.

Due to the different formulation used by \texttt{PMA:CCA} versus that of
\texttt{flashpcaR::scca} (constrained versus penalised, respectively),
it is not straightforward to compare the two models directly for a given
penalisation level. Hence, we compared the best predictive performance
achieved by the tools in cross-validation.  We used 3-fold cross-validation
over a~2D grid of $30\times25$ penalties, estimating one pair of canonical
vectors. The final predictive power was computed as the average Pearson
correlation $\bar{\rho}$ in the~$k=1,\hdots,3$ test folds:
$$
\bar{\rho} = \frac{1}{3} \sum_{k=1}^3
   \mbox{Cor}(\mathbf{X}_{\mbox{test}}^k u^k, \mathbf{Y}_{\mbox{test}}^k v^k).
$$
The maximum of the average test Pearson correlation was identical for
\texttt{flashpcaR::scca} and \texttt{PMA::CCA} ($\rho{=}0.936$), completing
in~5m and~24m, respectively (parallelising over 3 cores).


\section{Timing experiments}

For timing of \texttt{flashpcaR::scca} and \texttt{PMA::CCA}, we used
contiguous subsets of HapMap3 chromosome~1 (1000, 5000, 10,000, 20,000,
and 50,000 SNPs, out of~\nsnpschr SNPs in total) and contiguous subsets
of the~\ngenes real gene expression probes (1000, 10,000, and all~\ngenes
probes)~\citep{Stranger2012}.

We used the \textsf{R} package \texttt{microbenchmark}~\citep{Mersmann2015}
to run~30 replications of each timing experiment.  For all
experiments we estimated one pair of canonical vectors $(u_1,
v_1)$.  For the results in the main text, we initialised (``warm
started'') $v_1$ to a standard normally-distributed vector of variates
$\sim\mathcal{N}(0,1)$. \texttt{PMA::CCA} and \texttt{flashpcaR::scca}
allow the user to provide their own initialisation\footnote{The commandline
version \texttt{flashpca} currently only supports random initialisation.},
and we experimented with other forms, including using the column means
of the gene expression data and the rank-1 singular value decomposition
(SVD) $\mathbf{X}^T \mathbf{Y} \approx u_1 d_1 v_1^T$. The overall trend of
\texttt{flashpcaR} being several-fold faster than \texttt{PMA} was consistent
across all three initialistion methods~(Figure~\ref{fig:s01}).

\begin{figure}[!tpb]
\centering
\includegraphics[width=\textwidth]{scca_timing_full-crop.pdf}
\caption{
Timing (median of 30 runs) of SCCA implemented in the \texttt{flashpcaR}
(\textsf{R} package) and in \texttt{flashpca} (stand-alone commandline tool),
compared with SCCA from \texttt{PMA}, using subsets of the HapMap3 dataset with real
gene expression levels as phenotypes. We compared three schemes for initialising
$v_1$: (i) ``mean'': column means of the gene expression data; (ii) ``rand'':
normally-distributed variates $\mathcal{N}(0, 1)$; and (iii) ``svd'': 1st
right singular value of $\mathbf{X}^T \mathbf{Y}$. Note that the commandline
\texttt{flashpca} currently only supports random initialisation.
}
\label{fig:s01}
\end{figure}

All experiments were run in \textsf{R}~3.2.2~\citep{R} (with the original
LAPACK and BLAS libraries included in \textsf{R}) on 64-bit Ubuntu
Linux~12.04 on an Intel Xeon CPU~E7-4830 v2 @~2.20GHz. Time for the
commandline \texttt{flashpca} include loading of data into RAM and all
preprocessing (e.g., standardising the variables). We used flashpca~v1.2.6
(\url{https://github.com/gabraham/flashpca}) and PMA~v1.0.9~\citep{Witten2013}.
For \texttt{PMA::CCA}, we increased the maximum number of iterations to match
that used by \texttt{flashpcaR::scca} (default=1000), in order to prevent
early termination of the algorithm before adequate numerical convergence
was achieved.

\section{Parallelising grid search for penalty optimisation}

As described in Section~\ref{section:sim}, using multiple
cores can speed up the penalty grid search for \texttt{PMA} and
\texttt{flashpcaR}. Within \textsf{R}, this can be achieved using the
\texttt{foreach}~\citep{foreach} and \texttt{doMC}~\citep{doMC} packages. We
recommend using coarse-grain parallelisation for cross-validation, e.g.,
5 cores for 5-fold cross-validation. Examples are given in the code at
\url{https://github.com/gabraham/scca-paper}.

The commandline tool \texttt{flashpca} currently does not support built-in
cross-validation (as of v1.2.6). We recommend splitting the data into
training/test folds using PLINK and running \texttt{flashpca} on these subsets,
possibly using GNU parallel~\citep{Tange2011a}.



\bibliographystyle{unsrtnat}
\bibliography{paper}

\end{document}

